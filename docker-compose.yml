
services:
  # MongoDB - Stockage des données brutes
  mongo:
    image: mongo:6.0
    container_name: datalake-mongo
    restart: always
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
    volumes:
      - mongo_data:/data/db
      - ./mongo-init:/docker-entrypoint-initdb.d
    networks:
      - datalake

  # Mongo Express - Interface Web pour MongoDB
  mongo-express:
    image: mongo-express:latest
    container_name: datalake-mongo-express
    restart: always
    ports:
      - "8082:8081"
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongo
      - ME_CONFIG_MONGODB_PORT=27017
      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin
      - ME_CONFIG_MONGODB_ADMINPASSWORD=password123
      - ME_CONFIG_BASICAUTH_USERNAME=admin
      - ME_CONFIG_BASICAUTH_PASSWORD=admin
    depends_on:
      - mongo
    networks:
      - datalake

  # HDFS NameNode - Gestionnaire de métadonnées HDFS
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: datalake-namenode
    ports:
      - "9870:9870"  # Interface Web NameNode
      - "9000:9000"  # Port RPC NameNode
    environment:
      - CLUSTER_NAME=datalake-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - hadoop_namenode_data:/hadoop/dfs/name
    networks:
      - datalake

  # HDFS DataNode - Stockage des données
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datalake-datanode
    ports:
      - "9864:9864"  # Interface Web DataNode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - hadoop_datanode_data:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - datalake

  # Spark Master - Coordinateur du cluster Spark
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: datalake-spark-master
    ports:
      - "8086:8080"  # Interface Web Spark Master  
      - "7077:7077"  # Port du Spark Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./spark-apps:/spark-apps
    command: >
      bash -c "
        pip3 install pymongo requests &&
        /bin/bash /master.sh
      "
    depends_on:
      - namenode
    env_file:
      - .env
    networks:
      - datalake

  # Spark Worker - Nœud de traitement
  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: datalake-spark-worker
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8087
    ports:
      - "8087:8087"  # Interface Web Spark Worker
    volumes:
      - ./spark-apps:/spark-apps
    depends_on:
      - spark-master
    networks:
      - datalake

volumes:
  mongo_data:
  hadoop_namenode_data:
  hadoop_datanode_data:

networks:
  datalake:
    driver: bridge
