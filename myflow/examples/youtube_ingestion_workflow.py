"""
Workflow d'ingestion YouTube avec MyFlow
Pipeline complet: YouTube API -> MongoDB -> Spark -> HDFS
"""

import os
import sys
import subprocess
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'myflow_lib'))

from myflow_lib import DAG
from myflow_lib.operators.youtube_operator import (
    YouTubeExtractorOperator, 
    MongoDBHealthCheckOperator,
    HDFSHealthCheckOperator
)
from myflow_lib.operators.python_operator import PythonOperator


def check_environment():
    """VÃ©rifier que l'environnement est prÃªt"""
    required_vars = ['YOUTUBE_API_KEY', 'MONGO_URI']
    missing_vars = []
    
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"âŒ Variables d'environnement manquantes: {', '.join(missing_vars)}")
        return False
    
    print("âœ… Environnement configurÃ© correctement")
    return True


def run_spark_etl():
    """Appeler le script run_etl_spark.py pour exÃ©cuter l'ETL"""
    try:
        script_path = "/app/run_etl_spark.py"
        if not os.path.exists(script_path):
            print(f"âŒ Script ETL introuvable: {script_path}")
            return False
        
        print("âš¡ ExÃ©cution du script ETL Spark...")
        result = subprocess.run(
            ["python", script_path],
            check=True,
            text=True
        )
        print("âœ… ETL Spark exÃ©cutÃ© avec succÃ¨s")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Erreur lors de l'exÃ©cution du script ETL Spark: {e}")
        return False
    except Exception as e:
        print(f"âŒ Erreur inattendue: {e}")
        return False


def create_youtube_ingestion_dag():
    """CrÃ©er le DAG d'ingestion YouTube"""
    
    # CrÃ©er le DAG avec le Builder Pattern
    dag = (DAG.builder("youtube_ingestion_pipeline")
           .set_description("Pipeline d'ingestion YouTube: API -> MongoDB -> Spark -> HDFS")
           .add_tag("youtube")
           .add_tag("ingestion")
           .add_tag("etl")
           .build())
    
    # 1. VÃ©rification de l'environnement
    env_check = PythonOperator(
        "check_environment",
        "youtube_ingestion_pipeline",
        check_environment
    )
    
    # 2. VÃ©rification de la santÃ© des services
    mongo_health = MongoDBHealthCheckOperator(
        "mongo_health_check",
        "youtube_ingestion_pipeline"
    )
    
    hdfs_health = HDFSHealthCheckOperator(
        "hdfs_health_check", 
        "youtube_ingestion_pipeline"
    )
    
    # 3. Extraction YouTube vers MongoDB
    youtube_extraction = YouTubeExtractorOperator(
        "youtube_extraction",
        "youtube_ingestion_pipeline"
    )
    
    # 4. ETL Spark (MongoDB vers HDFS) via run_etl_spark.py
    spark_etl = PythonOperator(
        "spark_etl",
        "youtube_ingestion_pipeline",
        run_spark_etl
    )
    
    # Ajouter les tÃ¢ches au DAG
    dag.tasks["check_environment"] = env_check
    dag.tasks["mongo_health_check"] = mongo_health
    dag.tasks["hdfs_health_check"] = hdfs_health
    dag.tasks["youtube_extraction"] = youtube_extraction
    dag.tasks["spark_etl"] = spark_etl
    
    # DÃ©finir les dÃ©pendances
    dag.task_dependencies["check_environment"] = []
    dag.task_dependencies["mongo_health_check"] = ["check_environment"]
    dag.task_dependencies["hdfs_health_check"] = ["check_environment"]
    dag.task_dependencies["youtube_extraction"] = ["mongo_health_check"]
    dag.task_dependencies["spark_etl"] = ["youtube_extraction", "hdfs_health_check"]
    
    return dag


def main():
    """Fonction principale"""
    print("ğŸ¬ Workflow d'Ingestion YouTube avec MyFlow")
    print("=" * 60)
    
    # Charger les variables d'environnement depuis .env
    try:
        from dotenv import load_dotenv
        load_dotenv(os.path.join(os.path.dirname(__file__), '..', '..', '.env'))
        print("âœ… Variables d'environnement chargÃ©es depuis .env")
    except ImportError:
        print("âš ï¸ python-dotenv non installÃ©, utilisation des variables systÃ¨me")
    except Exception as e:
        print(f"âš ï¸ Erreur lors du chargement .env: {e}")
    
    # CrÃ©er le DAG
    dag = create_youtube_ingestion_dag()
    
    # Valider le DAG
    if not dag.validate():
        print("âŒ DAG invalide!")
        return
    
    print("âœ… DAG validÃ© avec succÃ¨s")
    print(f"ğŸ“‹ TÃ¢ches: {list(dag.tasks.keys())}")
    print(f"ğŸ”„ Ordre d'exÃ©cution: {' -> '.join(dag.get_execution_order())}")
    
    # ExÃ©cuter le workflow
    print("\nğŸ”„ ExÃ©cution du workflow d'ingestion...")
    
    for task_id in dag.get_execution_order():
        task = dag.tasks[task_id]
        print(f"\nğŸ“ ExÃ©cution de {task_id}...")
        
        success = task.run()
        status = "âœ… SuccÃ¨s" if success else "âŒ Ã‰chec"
        print(f"   {status}")
        
        if not success:
            print(f"âŒ Workflow arrÃªtÃ© Ã  cause de l'Ã©chec de {task_id}")
            break
    
    # RÃ©sumÃ© final
    print("\nğŸ“Š RÃ©sumÃ© du workflow:")
    summary = dag.get_status_summary()
    for state, count in summary.items():
        if count > 0:
            print(f"   {state.capitalize()}: {count}")
    
    # VÃ©rifier le succÃ¨s global
    if dag.is_complete() and not dag.has_failures():
        print("\nğŸ‰ Workflow d'ingestion YouTube terminÃ© avec succÃ¨s!")
    else:
        print("\nğŸ’¥ Workflow d'ingestion YouTube a Ã©chouÃ©!")


if __name__ == "__main__":
    main()
