#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script simple pour alimenter Metabase avec les donn√©es HDFS via Spark
"""

import sys
import logging
import time
import subprocess
from datetime import datetime

# Configuration des chemins Spark
sys.path.append('/spark/python')
sys.path.append('/spark/python/lib/py4j-0.10.9.5-src.zip')

from pyspark.sql import SparkSession
from pyspark.sql.functions import count, sum as spark_sum, avg, desc, current_timestamp
from pyspark.sql.types import *

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleHDFSToMetabase:
    def __init__(self):
        self.spark = None
        self.hdfs_path = "hdfs://namenode:9000/data/youtube"
        
        # Configuration PostgreSQL
        self.jdbc_url = "jdbc:postgresql://postgres:5432/metabase"
        self.jdbc_properties = {
            "user": "metabase",
            "password": "metabase123",
            "driver": "org.postgresql.Driver"
        }
    
    def setup_spark(self):
        """Initialise Spark avec PostgreSQL"""
        try:
            # T√©l√©chargement du driver PostgreSQL
            logger.info("üì¶ T√©l√©chargement du driver PostgreSQL...")
            subprocess.run([
                "wget", "-O", "/spark/jars/postgresql-42.6.0.jar",
                "https://jdbc.postgresql.org/download/postgresql-42.6.0.jar"
            ], check=True, capture_output=True)
            
            self.spark = SparkSession.builder \
                .appName("Simple_HDFS_to_Metabase") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
                .config("spark.jars", "/spark/jars/postgresql-42.6.0.jar") \
                .getOrCreate()
            
            logger.info("‚úÖ Spark configur√© avec succ√®s")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur configuration Spark: {e}")
            return False
    
    def check_hdfs_data(self):
        """V√©rifie la disponibilit√© des donn√©es HDFS"""
        try:
            # Test de lecture des donn√©es influenceurs
            df = self.spark.read.parquet(f"{self.hdfs_path}/influenceur")
            count = df.count()
            
            logger.info(f"‚úÖ Donn√©es trouv√©es: {count} influenceurs")
            logger.info("üìä Aper√ßu des donn√©es:")
            df.select("influencer_name", "main_niche", "total_views", "engagement_rate") \
              .show(5, truncate=False)
            
            return True, df
        except Exception as e:
            logger.error(f"‚ùå Donn√©es HDFS non trouv√©es: {e}")
            return False, None
    
    def transfer_to_postgres(self, df):
        """Transf√®re les donn√©es vers PostgreSQL pour Metabase"""
        try:
            logger.info("üîÑ Transfert des donn√©es vers PostgreSQL...")
            
            # Nettoyage et enrichissement des donn√©es
            df_clean = df.withColumn("import_date", current_timestamp()) \
                        .fillna(0, subset=["total_views", "engagement_rate", "total_videos"])
            
            # Transfer principal
            df_clean.write \
                .mode("overwrite") \
                .option("truncate", "true") \
                .jdbc(self.jdbc_url, "youtube_influencers", properties=self.jdbc_properties)
            
            logger.info("‚úÖ Table 'youtube_influencers' cr√©√©e")
            
            # Cr√©er des vues analytics
            self._create_analytics_views(df_clean)
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur transfert: {e}")
            return False
    
    def _create_analytics_views(self, df):
        """Cr√©e des vues d'analyse pour faciliter l'utilisation dans Metabase"""
        try:
            # Stats par niche
            df_niche = df.groupBy("main_niche") \
                .agg(
                    count("*").alias("nombre_influenceurs"),
                    spark_sum("total_views").alias("total_vues"),
                    avg("engagement_rate").alias("engagement_moyen"),
                    avg("total_videos").alias("nb_videos_moyen")
                ) \
                .withColumn("created_at", current_timestamp())
            
            df_niche.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "analytics_par_niche", properties=self.jdbc_properties)
            
            # Top influenceurs
            df_top = df.orderBy(desc("total_views")) \
                .limit(50) \
                .withColumn("created_at", current_timestamp())
            
            df_top.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "top_influenceurs", properties=self.jdbc_properties)
            
            # Stats par tier d'influence
            df_tier = df.groupBy("influence_tier") \
                .agg(
                    count("*").alias("nombre_influenceurs"),
                    spark_sum("total_views").alias("total_vues"),
                    avg("engagement_rate").alias("engagement_moyen")
                ) \
                .withColumn("created_at", current_timestamp())
            
            df_tier.write \
                .mode("overwrite") \
                .jdbc(self.jdbc_url, "analytics_par_tier", properties=self.jdbc_properties)
            
            logger.info("‚úÖ Vues analytics cr√©√©es")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur cr√©ation vues analytics: {e}")
    
    def verify_data(self):
        """V√©rifie les donn√©es dans PostgreSQL"""
        try:
            # V√©rification table principale
            df_check = self.spark.read.jdbc(self.jdbc_url, "youtube_influencers", properties=self.jdbc_properties)
            count = df_check.count()
            
            logger.info(f"‚úÖ V√©rification: {count} enregistrements dans PostgreSQL")
            
            # Aper√ßu des analytics
            df_niche = self.spark.read.jdbc(self.jdbc_url, "analytics_par_niche", properties=self.jdbc_properties)
            logger.info("üìà Statistiques par niche:")
            df_niche.select("main_niche", "nombre_influenceurs", "total_vues") \
                   .orderBy(desc("total_vues")) \
                   .show(10, truncate=False)
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification: {e}")
            return False
    
    def run(self):
        """Ex√©cute le processus complet"""
        logger.info("üöÄ D√âMARRAGE: HDFS ‚Üí Metabase via Spark")
        logger.info("=" * 50)
        
        # Configuration Spark
        if not self.setup_spark():
            return False
        
        # Attente stabilisation des services
        logger.info("‚è≥ Attente stabilisation des services (30s)...")
        time.sleep(30)
        
        # V√©rification donn√©es HDFS
        success, df = self.check_hdfs_data()
        if not success:
            logger.error("‚ùå Impossible de lire les donn√©es HDFS")
            return False
        
        # Transfert vers PostgreSQL
        if not self.transfer_to_postgres(df):
            return False
        
        # V√©rification finale
        if not self.verify_data():
            return False
        
        # Informations de connexion
        logger.info("")
        logger.info("üéâ SUCC√àS! Donn√©es disponibles dans Metabase")
        logger.info("=" * 50)
        logger.info("üìä TABLES CR√â√âES:")
        logger.info("  ‚Ä¢ youtube_influencers    - Donn√©es principales")
        logger.info("  ‚Ä¢ analytics_par_niche    - Stats par niche")
        logger.info("  ‚Ä¢ analytics_par_tier     - Stats par tier")
        logger.info("  ‚Ä¢ top_influenceurs       - Top 50 influenceurs")
        logger.info("")
        logger.info("üîó ACC√àS METABASE:")
        logger.info("  ‚Ä¢ URL: http://localhost:3000")
        logger.info("  ‚Ä¢ Type: PostgreSQL")
        logger.info("  ‚Ä¢ Host: postgres")
        logger.info("  ‚Ä¢ Port: 5432")
        logger.info("  ‚Ä¢ Database: metabase")
        logger.info("  ‚Ä¢ User: metabase")
        logger.info("  ‚Ä¢ Password: metabase123")
        
        self.spark.stop()
        return True

def main():
    """Point d'entr√©e principal"""
    logger.info("üîß Initialisation du connecteur HDFS ‚Üí Metabase")
    
    connector = SimpleHDFSToMetabase()
    success = connector.run()
    
    if success:
        logger.info("‚úÖ Processus termin√© avec succ√®s")
        exit(0)
    else:
        logger.error("‚ùå Processus √©chou√©")
        exit(1)

if __name__ == "__main__":
    main()
