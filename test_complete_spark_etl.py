#!/usr/bin/env python3
"""
üß™ Test du script Spark MongoDB ‚Üí HDFS Complet
Ex√©cute le script complet dans le conteneur Spark Master
"""

import subprocess
import logging
import sys
import os
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_complete_spark_etl():
    """Test du script ETL complet dans le conteneur Spark"""
    logger.info("üéØ TEST SCRIPT ETL COMPLET - EX√âCUTION DANS CONTENEUR SPARK")
    logger.info("=" * 60)
    
    try:
        # 1. V√©rification des services Docker
        logger.info("üîç V√©rification des services...")
        
        # MongoDB
        mongo_check = subprocess.run(
            "docker exec datalake-mongo mongosh -u admin -p password123 --authenticationDatabase admin --eval 'db.runCommand(\"ping\")' --quiet",
            shell=True, capture_output=True, text=True
        )
        if mongo_check.returncode != 0:
            logger.error("‚ùå MongoDB non accessible")
            return False
        logger.info("‚úÖ MongoDB accessible")
        
        # HDFS
        hdfs_check = subprocess.run(
            "docker exec datalake-namenode hdfs dfsadmin -report",
            shell=True, capture_output=True, text=True
        )
        if hdfs_check.returncode != 0:
            logger.error("‚ùå HDFS non accessible")
            return False
        logger.info("‚úÖ HDFS accessible")
        
        # Spark Master
        spark_check = subprocess.run(
            "docker exec datalake-spark-master ps aux | grep -q 'Master'",
            shell=True, capture_output=True, text=True
        )
        if spark_check.returncode != 0:
            logger.error("‚ùå Spark Master non accessible")
            return False
        logger.info("‚úÖ Spark Master accessible")
        
        # 2. Copie du script dans le conteneur Spark
        logger.info("üìÅ Copie du script ETL complet dans le conteneur...")
        
        copy_script = subprocess.run(
            "docker cp mongo_to_hdfs_spark_complete.py datalake-spark-master:/spark/",
            shell=True, capture_output=True, text=True
        )
        
        if copy_script.returncode != 0:
            logger.error(f"‚ùå Erreur copie script: {copy_script.stderr}")
            return False
        logger.info("‚úÖ Script ETL copi√© dans le conteneur")
        
        # 3. Installation des d√©pendances dans le conteneur
        logger.info("üì¶ V√©rification/Installation des d√©pendances...")
        
        # Installation de PyMongo
        install_pymongo = subprocess.run(
            "docker exec datalake-spark-master pip install pymongo",
            shell=True, capture_output=True, text=True
        )
        if install_pymongo.returncode == 0:
            logger.info("‚úÖ PyMongo install√©/v√©rifi√©")
        else:
            logger.warning(f"‚ö†Ô∏è Avertissement PyMongo: {install_pymongo.stderr}")
        
        # 4. V√©rification des donn√©es MongoDB
        logger.info("üìä V√©rification des donn√©es source...")
        count_result = subprocess.run(
            "docker exec datalake-mongo mongosh -u admin -p password123 --authenticationDatabase admin datalake --eval 'db.raw_data.countDocuments()' --quiet",
            shell=True, capture_output=True, text=True
        )
        
        if count_result.returncode == 0:
            try:
                doc_count = int(count_result.stdout.strip())
                logger.info(f"üìä Documents dans MongoDB: {doc_count}")
                if doc_count == 0:
                    logger.warning("‚ö†Ô∏è Aucune donn√©e dans MongoDB - Le script utilisera des donn√©es de d√©mo")
                else:
                    logger.info("‚úÖ Donn√©es r√©elles disponibles dans MongoDB")
            except ValueError:
                logger.warning("‚ö†Ô∏è Impossible de compter les documents MongoDB")
        else:
            logger.warning("‚ö†Ô∏è Erreur acc√®s MongoDB")
        
        # 5. Ex√©cution du script ETL complet dans le conteneur
        logger.info("‚ö° LANCEMENT DU SCRIPT ETL COMPLET...")
        logger.info("üéØ Reproduction fid√®le du notebook YouTube ETL Pipeline")
        start_time = datetime.now()
        
        spark_execution = subprocess.run(
            "docker exec datalake-spark-master python /spark/mongo_to_hdfs_spark_complete.py",
            shell=True, capture_output=True, text=True, timeout=1800  # 30 minutes timeout
        )
        
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        # 6. Analyse des r√©sultats
        if spark_execution.returncode == 0:
            logger.info(f"‚úÖ Script ETL COMPLET termin√© en {execution_time:.2f}s")
            logger.info("üìÑ OUTPUT DU SCRIPT:")
            print("=" * 60)
            print(spark_execution.stdout)
            print("=" * 60)
            
            # V√©rification d√©taill√©e HDFS
            logger.info("üîç V√©rification d√©taill√©e des donn√©es HDFS...")
            
            # Liste des r√©pertoires cr√©√©s
            hdfs_structure = subprocess.run(
                "docker exec datalake-namenode hdfs dfs -ls -R /data/youtube/",
                shell=True, capture_output=True, text=True
            )
            
            if hdfs_structure.returncode == 0:
                logger.info("‚úÖ Structure HDFS compl√®te cr√©√©e:")
                print(hdfs_structure.stdout)
                
                # V√©rification des fichiers Parquet
                logger.info("üìä V√©rification des fichiers de donn√©es...")
                
                # Donn√©es principales
                main_data_check = subprocess.run(
                    "docker exec datalake-namenode hdfs dfs -count /data/youtube/influenceur",
                    shell=True, capture_output=True, text=True
                )
                if main_data_check.returncode == 0:
                    logger.info("‚úÖ Donn√©es principales sauvegard√©es")
                    logger.info(f"üìÑ {main_data_check.stdout.strip()}")
                
                # M√©tadonn√©es
                metadata_check = subprocess.run(
                    "docker exec datalake-namenode hdfs dfs -count /data/youtube/metadata",
                    shell=True, capture_output=True, text=True
                )
                if metadata_check.returncode == 0:
                    logger.info("‚úÖ M√©tadonn√©es sauvegard√©es")
                    logger.info(f"üìÑ {metadata_check.stdout.strip()}")
                
            else:
                logger.warning("‚ö†Ô∏è Impossible de lister la structure HDFS compl√®te")
            
            # Test de lecture des donn√©es sauvegard√©es
            logger.info("üîç Test de lecture des donn√©es depuis HDFS...")
            read_test = subprocess.run(
                "docker exec datalake-spark-master python -c \"from pyspark.sql import SparkSession; spark = SparkSession.builder.appName('ReadTest').config('spark.hadoop.fs.defaultFS', 'hdfs://datalake-namenode:9000').getOrCreate(); df = spark.read.parquet('/data/youtube/influenceur'); print(f'Lignes lues: {df.count()}'); spark.stop()\"",
                shell=True, capture_output=True, text=True, timeout=120
            )
            
            if read_test.returncode == 0:
                logger.info("‚úÖ Test de lecture HDFS r√©ussi:")
                logger.info(f"üìä {read_test.stdout.strip()}")
            else:
                logger.warning(f"‚ö†Ô∏è Erreur test de lecture: {read_test.stderr}")
            
            return True
            
        else:
            logger.error("‚ùå √âchec du script ETL complet")
            logger.error("üìÑ ERREUR:")
            print("=" * 60)
            print(spark_execution.stderr)
            print("=" * 60)
            logger.error("üìÑ OUTPUT PARTIEL:")
            print(spark_execution.stdout)
            return False
            
    except subprocess.TimeoutExpired:
        logger.error("‚è∞ Timeout du script ETL (30 minutes)")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return False

def main():
    """Point d'entr√©e principal"""
    success = test_complete_spark_etl()
    
    if success:
        print("\n" + "="*60)
        print("üéâ TEST ETL COMPLET R√âUSSI!")
        print("="*60)
        print("üåê Interfaces de v√©rification:")
        print("   ‚Ä¢ HDFS NameNode: http://localhost:9870")
        print("   ‚Ä¢ Spark Master: http://localhost:8086")
        print("   ‚Ä¢ MongoDB Express: http://localhost:8082")
        print("   ‚Ä¢ Jupyter Lab: http://localhost:8888")
        print("\nüìÅ Donn√©es disponibles dans HDFS:")
        print("   ‚Ä¢ /data/youtube/influenceur/ (partitionn√©)")
        print("   ‚Ä¢ /data/youtube/metadata/")
        print("   ‚Ä¢ /data/youtube/influenceur/niche_reference/")
        print("   ‚Ä¢ /data/youtube/influenceur/geo_reference/")
        print("\n‚úÖ Pipeline Data Lakehouse op√©rationnelle!")
        sys.exit(0)
    else:
        print("\n" + "="*60)
        print("‚ùå TEST ETL COMPLET √âCHOU√â!")
        print("="*60)
        print("üí° Actions de d√©bogage:")
        print("   1. V√©rifiez les logs: docker-compose logs [service]")
        print("   2. V√©rifiez l'espace disque: docker system df")
        print("   3. Red√©marrez les services: docker-compose restart")
        print("   4. V√©rifiez la connectivit√© r√©seau des conteneurs")
        sys.exit(1)

if __name__ == "__main__":
    main()
