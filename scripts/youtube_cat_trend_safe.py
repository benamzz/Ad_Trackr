#!/usr/bin/env python3
"""
Script S√âCURIS√â pour 30-50 mots-cl√©s YouTube - Anti-429 optimis√©
STRAT√âGIE: Requ√™tes s√©quentielles intelligentes avec gestion adaptative
"""

import os
import json
import time
import random
import hashlib
from datetime import datetime, timedelta
from pytrends.request import TrendReq
from pymongo import MongoClient

# Configuration S√âCURIS√âE anti-429
DELAY_MIN = 12  # D√©lai minimum augment√©
DELAY_MAX = 20  # D√©lai maximum augment√©
DELAY_AFTER_ERROR = 60  # 1 minute apr√®s erreur 429
MAX_RETRIES = 5  # Plus de tentatives
BATCH_PAUSE_EVERY = 10  # Pause longue tous les 10 mots-cl√©s
BATCH_PAUSE_DURATION = 120  # 2 minutes de pause

# Pool User-Agents √©tendu pour rotation maximale
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:118.0) Gecko/20100101 Firefox/118.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/121.0'
]

# G√©olocalisations
AVAILABLE_GEOS = {
    'FR': 'France', 'US': '√âtats-Unis', 'GB': 'Royaume-Uni', 'DE': 'Allemagne',
    'ES': 'Espagne', 'IT': 'Italie', 'CA': 'Canada', 'AU': 'Australie',
    'JP': 'Japon', 'BR': 'Br√©sil', 'IN': 'Inde', 'CN': 'Chine', '': 'Mondial'
}

# Cat√©gories YouTube pr√©d√©finies
YOUTUBE_CATEGORIES = {
    'gaming': ['gaming', 'esport', 'minecraft', 'fortnite', 'valorant', 'league of legends'],
    'beauty': ['makeup', 'skincare', 'beauty haul', 'cosmetics', 'tutorial makeup', 'beaut√©'],
    'tech': ['tech review', 'smartphone', 'laptop', 'AI', 'technologie', 'innovation'],
    'fitness': ['fitness', 'workout', 'sport', 'musculation', 'yoga', 'running'],
    'food': ['recette', 'cooking', 'food', 'cuisine', 'chef', 'gastronomie'],
    'music': ['music', 'concert', 'streaming', 'musique', 'chanson', 'artiste'],
    'education': ['tutorial', 'learning', 'education', 'cours', 'formation', 'apprentissage'],
    'lifestyle': ['vlog', 'lifestyle', 'travel', 'decoration', 'diy', 'voyage']
}

class QuotaManager:
    """Gestionnaire intelligent des quotas Google Trends"""
    
    def __init__(self):
        self.requests_made = 0
        self.session_start = datetime.now()
        self.last_error_time = None
        self.consecutive_errors = 0
        
    def get_adaptive_delay(self):
        """D√©lai adaptatif bas√© sur les erreurs r√©centes"""
        base_delay = random.uniform(DELAY_MIN, DELAY_MAX)
        
        # Augmenter le d√©lai si erreurs r√©centes
        if self.consecutive_errors > 0:
            multiplier = 1 + (self.consecutive_errors * 0.5)
            base_delay *= multiplier
            
        # D√©lai plus long en fin de session
        if self.requests_made > 20:
            base_delay *= 1.5
            
        return base_delay
    
    def should_take_batch_pause(self):
        """D√©termine si une pause longue est n√©cessaire"""
        return self.requests_made > 0 and self.requests_made % BATCH_PAUSE_EVERY == 0
    
    def record_success(self):
        """Enregistre une requ√™te r√©ussie"""
        self.requests_made += 1
        self.consecutive_errors = 0
        
    def record_error(self):
        """Enregistre une erreur 429"""
        self.consecutive_errors += 1
        self.last_error_time = datetime.now()
        
    def get_error_recovery_delay(self):
        """D√©lai de r√©cup√©ration apr√®s erreur 429"""
        if self.consecutive_errors == 1:
            return DELAY_AFTER_ERROR
        elif self.consecutive_errors == 2:
            return DELAY_AFTER_ERROR * 2
        else:
            return DELAY_AFTER_ERROR * 3  # D√©lai maximal

def get_random_user_agent():
    """User-Agent al√©atoire avec rotation maximale"""
    return random.choice(USER_AGENTS)

def create_cache_key(keyword, geo, timeframe):
    """Cl√© de cache unique"""
    cache_string = f"{keyword}_{geo}_{timeframe}"
    return hashlib.md5(cache_string.encode()).hexdigest()

def connect_mongodb():
    """Connexion MongoDB"""
    try:
        mongo_uri = os.getenv('MONGO_URI', 'mongodb://localhost:27017')
        client = MongoClient(mongo_uri)
        db = client['ad_trackr']
        client.admin.command('ping')
        return db
    except Exception as e:
        print(f"‚ùå Erreur MongoDB: {e}")
        return None

def check_cached_data(keyword, geo, timeframe, max_age_hours=48):
    """Cache avec dur√©e √©tendue pour √©viter re-requ√™tes"""
    try:
        db = connect_mongodb()
        if not db:
            return None
            
        cache_key = create_cache_key(keyword, geo, timeframe)
        cached_doc = db.google_trends_raw.find_one({'cache_key': cache_key})
        
        if cached_doc:
            cache_time = datetime.fromisoformat(cached_doc.get('ingestion_timestamp'))
            if datetime.now() - cache_time < timedelta(hours=max_age_hours):
                return cached_doc
        return None
    except Exception:
        return None

def get_single_keyword_trends_safe(keyword, geo, timeframe, quota_manager):
    """
    Version S√âCURIS√âE pour un seul mot-cl√© avec gestion adaptive
    """
    # V√©rifier cache en priorit√©
    cached = check_cached_data(keyword, geo, timeframe)
    if cached:
        print(f"üéØ Cache utilis√©: '{keyword}' ({geo})")
        return cached
    
    print(f"üîç API requ√™te: '{keyword}' ({geo}) [Req #{quota_manager.requests_made + 1}]")
    
    for attempt in range(MAX_RETRIES):
        try:
            # D√©lai adaptatif avant requ√™te
            if attempt > 0:
                recovery_delay = quota_manager.get_error_recovery_delay()
                print(f"‚è∞ R√©cup√©ration erreur: {recovery_delay}s (tentative {attempt + 1})")
                time.sleep(recovery_delay)
            elif quota_manager.requests_made > 0:
                adaptive_delay = quota_manager.get_adaptive_delay()
                print(f"‚è∞ D√©lai adaptatif: {adaptive_delay:.1f}s")
                time.sleep(adaptive_delay)
            
            # Pause longue si n√©cessaire
            if quota_manager.should_take_batch_pause():
                print(f"üõë PAUSE BATCH ({BATCH_PAUSE_DURATION}s) - Protection quota")
                time.sleep(BATCH_PAUSE_DURATION)
            
            # PyTrends avec configuration conservatrice
            user_agent = get_random_user_agent()
            pytrends = TrendReq(
                hl='fr', tz=360,
                user_agent=user_agent,
                retries=0,  # Pas de retry automatique (on g√®re manuellement)
                backoff_factor=0,
                timeout=(10, 20)  # Timeout plus long
            )
            
            # Requ√™te individuelle pour valeurs absolues
            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo=geo, gprop='')
            
            # D√©lai avant r√©cup√©ration des donn√©es
            time.sleep(random.uniform(1, 3))
            
            interest_over_time = pytrends.interest_over_time()
            
            if interest_over_time.empty:
                print(f"‚ùå Pas de donn√©es: '{keyword}'")
                quota_manager.record_success()  # Pas une erreur 429
                return None
            
            # Conversion format brut
            raw_data = []
            for index, row in interest_over_time.iterrows():
                raw_data.append({
                    'date': index.isoformat(),
                    'value': int(row[keyword])
                })
            
            # Analyse des pics
            values = [point['value'] for point in raw_data]
            max_value = max(values) if values else 0
            peak_dates = [point['date'] for point in raw_data if point['value'] == max_value]
            
            # Calcul des statistiques de campagne
            high_interest_periods = [point for point in raw_data if point['value'] >= 70]
            avg_value = sum(values) / len(values) if values else 0
            
            result = {
                'keyword': keyword,
                'geo': geo,
                'geo_name': AVAILABLE_GEOS.get(geo, geo),
                'timeframe': timeframe,
                'ingestion_timestamp': datetime.now().isoformat(),
                'raw_data': raw_data,
                'source': 'google_trends_api_safe',
                'cache_key': create_cache_key(keyword, geo, timeframe),
                'user_agent_used': user_agent,
                'attempt_number': attempt + 1,
                # Analyse campagne
                'peak_value': max_value,
                'peak_dates': peak_dates,
                'high_interest_count': len(high_interest_periods),
                'average_interest': round(avg_value, 1),
                'data_points_count': len(raw_data),
                'request_number': quota_manager.requests_made + 1
            }
            
            # Succ√®s !
            quota_manager.record_success()
            print(f"‚úÖ '{keyword}': {len(raw_data)} points, pic={max_value}, moy={avg_value:.1f}")
            return result
            
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "Too Many Requests" in error_msg:
                quota_manager.record_error()
                print(f"üö® ERREUR 429 '{keyword}' (tentative {attempt + 1}/{MAX_RETRIES})")
                print(f"   üìä Erreurs cons√©cutives: {quota_manager.consecutive_errors}")
                
                if attempt < MAX_RETRIES - 1:
                    # D√©lai exponentiel apr√®s 429
                    recovery_time = quota_manager.get_error_recovery_delay()
                    print(f"   ‚è∞ Attente r√©cup√©ration: {recovery_time}s")
                    time.sleep(recovery_time)
                else:
                    print(f"   ‚ùå Abandon '{keyword}' - Quota probablement √©puis√©")
                    return None
            else:
                print(f"‚ùå Erreur technique '{keyword}': {e}")
                time.sleep(5)  # Pause courte pour erreurs techniques
                
                if attempt == MAX_RETRIES - 1:
                    return None
    
    return None

def save_to_mongodb(result):
    """Sauvegarde individuelle avec gestion d'erreurs"""
    try:
        db = connect_mongodb()
        if not db:
            return False
        
        collection = db.google_trends_raw
        
        # Upsert par cache_key
        collection.replace_one(
            {'cache_key': result['cache_key']},
            result,
            upsert=True
        )
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde '{result['keyword']}': {e}")
        return False

def analyze_campaign_opportunities(results):
    """Analyse d√©taill√©e des opportunit√©s de campagne"""
    if not results:
        return
    
    print("\n" + "="*80)
    print("üìà ANALYSE DES OPPORTUNIT√âS DE CAMPAGNE")
    print("="*80)
    
    successful_results = [r for r in results if r]
    
    # Tri par potentiel de campagne (pic + fr√©quence)
    sorted_results = sorted(
        successful_results, 
        key=lambda x: (x['peak_value'], x['high_interest_count']), 
        reverse=True
    )
    
    for i, result in enumerate(sorted_results, 1):
        keyword = result['keyword']
        peak_value = result['peak_value']
        avg_interest = result['average_interest']
        high_count = result['high_interest_count']
        
        print(f"\nüèÜ RANG #{i}: {keyword.upper()}")
        print(f"   üìä Pic maximum: {peak_value}/100")
        print(f"   üìà Int√©r√™t moyen: {avg_interest}/100")
        print(f"   üéØ P√©riodes favorables: {high_count} points ‚â•70")
        
        # Recommandation strat√©gique
        if peak_value >= 90 and high_count >= 5:
            recommendation = "üåü PRIORIT√â MAXIMALE - Excellent potentiel"
        elif peak_value >= 80 and high_count >= 3:
            recommendation = "‚≠ê HAUTE PRIORIT√â - Bon potentiel"
        elif peak_value >= 70:
            recommendation = "‚ú® PRIORIT√â MOD√âR√âE - Potentiel correct"
        else:
            recommendation = "üí´ PRIORIT√â FAIBLE - Potentiel limit√©"
        
        print(f"   üí° {recommendation}")
        
        # Analyse temporelle des pics
        if result['peak_dates']:
            print(f"   üìÖ Meilleurs moments:")
            for date_str in result['peak_dates'][:3]:  # Top 3
                date_obj = datetime.fromisoformat(date_str)
                month_year = date_obj.strftime("%B %Y")
                print(f"      üóìÔ∏è {month_year}")

def estimate_completion_time(total_keywords, quota_manager):
    """Estimation du temps restant"""
    if quota_manager.requests_made == 0:
        avg_time_per_request = 20  # Estimation initiale
    else:
        elapsed = (datetime.now() - quota_manager.session_start).total_seconds()
        avg_time_per_request = elapsed / quota_manager.requests_made
    
    remaining = total_keywords - quota_manager.requests_made
    estimated_seconds = remaining * avg_time_per_request
    
    hours = int(estimated_seconds // 3600)
    minutes = int((estimated_seconds % 3600) // 60)
    
    if hours > 0:
        return f"{hours}h {minutes}min"
    else:
        return f"{minutes}min"

def main():
    """Fonction principale S√âCURIS√âE"""
    print("üõ°Ô∏è YOUTUBE CAMPAIGN ANALYZER - VERSION S√âCURIS√âE")
    print("=" * 70)
    print("üéØ Optimis√© pour 30-50 mots-cl√©s avec protection quota Google Trends")
    
    try:
        # Initialisation
        quota_manager = QuotaManager()
        db = connect_mongodb()
        if not db:
            return
        print("‚úÖ MongoDB connect√©")
        
        # Configuration g√©o
        print(f"\nüåç G√âOLOCALISATIONS PRINCIPALES:")
        main_geos = ['FR', 'US', 'GB', 'DE', 'ES', 'IT']
        for code in main_geos:
            print(f"  {code:3} ‚Üí {AVAILABLE_GEOS[code]}")
        
        geo_input = input("\nCode g√©o (FR par d√©faut): ").strip().upper()
        geo = geo_input if geo_input in AVAILABLE_GEOS else 'FR'
        print(f"‚úÖ G√©olocalisation: {geo} ({AVAILABLE_GEOS[geo]})")
        
        # Saisie mots-cl√©s
        print(f"\nüé¨ CAT√âGORIES YOUTUBE DISPONIBLES:")
        for category in YOUTUBE_CATEGORIES.keys():
            print(f"   üìÇ {category}")
        
        print(f"\nüí° EXEMPLES D'UTILISATION:")
        print("   üî∏ Cat√©gories: gaming, beauty, tech")
        print("   üî∏ Mots-cl√©s: minecraft, makeup, smartphone, fitness")
        print("   üî∏ Mixte: gaming, makeup tutorial, tech review")
        
        keywords_input = input("\nMots-cl√©s/cat√©gories (virgules): ").strip()
        
        if not keywords_input:
            print("‚ùå Aucune saisie")
            return
        
        # Expansion et pr√©paration
        keywords = []
        for item in keywords_input.split(','):
            item = item.strip().lower()
            if item in YOUTUBE_CATEGORIES:
                expanded = YOUTUBE_CATEGORIES[item]
                keywords.extend(expanded)
                print(f"üìÇ '{item}' ‚Üí {len(expanded)} mots-cl√©s")
            else:
                keywords.append(item)
        
        # Nettoyage et limitation
        keywords = list(dict.fromkeys(keywords))  # D√©doublonnage
        if len(keywords) > 50:
            print(f"‚ö†Ô∏è Limitation √† 50 mots-cl√©s (vous aviez {len(keywords)})")
            keywords = keywords[:50]
        
        print(f"\nüìã LISTE FINALE: {len(keywords)} mots-cl√©s")
        
        # Estimation temps
        estimated_time = len(keywords) * 18  # 18 secondes en moyenne par mot-cl√©
        hours = estimated_time // 3600
        minutes = (estimated_time % 3600) // 60
        time_str = f"{hours}h {minutes}min" if hours > 0 else f"{minutes}min"
        
        print(f"‚è∞ Temps estim√©: {time_str}")
        print(f"üõ°Ô∏è Protection 429: D√©lais adaptatifs + pauses automatiques")
        
        # Confirmation
        confirm = input(f"\nüöÄ Lancer l'analyse S√âCURIS√âE? (y/n): ")
        if confirm.lower() not in ['y', 'yes', 'o', 'oui']:
            return
        
        # Traitement s√©quentiel s√©curis√©
        print(f"\nüîÑ TRAITEMENT S√âCURIS√â EN COURS...")
        results = []
        
        for i, keyword in enumerate(keywords, 1):
            print(f"\n--- [{i}/{len(keywords)}] ---")
            
            # Estimation temps restant
            if i > 1:
                remaining_time = estimate_completion_time(len(keywords), quota_manager)
                print(f"‚è∞ Temps restant estim√©: {remaining_time}")
            
            # Traitement avec protection
            result = get_single_keyword_trends_safe(keyword, geo, 'today 12-m', quota_manager)
            
            if result:
                # Sauvegarde imm√©diate
                if save_to_mongodb(result):
                    print(f"üíæ Sauv√©: '{keyword}'")
                results.append(result)
            else:
                results.append(None)
            
            # V√©rification √©tat quota
            if quota_manager.consecutive_errors >= 3:
                print(f"üö® ALERTE: {quota_manager.consecutive_errors} erreurs cons√©cutives")
                print(f"üí° Recommandation: Pause longue ou arr√™t")
                
                pause_choice = input("Continuer (c), Pause 5min (p), ou Arr√™ter (a)? ")
                if pause_choice.lower() == 'a':
                    break
                elif pause_choice.lower() == 'p':
                    print("‚è∏Ô∏è Pause de 5 minutes...")
                    time.sleep(300)
                    quota_manager.consecutive_errors = 0
        
        # Analyse finale
        analyze_campaign_opportunities(results)
        
        # Statistiques
        successful = len([r for r in results if r])
        duration = (datetime.now() - quota_manager.session_start).total_seconds()
        
        print(f"\nüéØ R√âSULTATS FINAUX:")
        print(f"‚úÖ Succ√®s: {successful}/{len(keywords)} mots-cl√©s")
        print(f"‚è±Ô∏è Dur√©e totale: {duration/60:.1f} minutes")
        print(f"üö® Erreurs 429: {quota_manager.requests_made - successful}")
        print(f"üìä Taux de succ√®s: {(successful/len(keywords)*100):.1f}%")
        
        if successful > 0:
            print(f"\nüöÄ Donn√©es pr√™tes pour:")
            print(f"   üìä Spark processing (spark_mongo_to_hdfs_optimized.py)")
            print(f"   üìà Power BI export (powerbi_export.py)")
        
    except KeyboardInterrupt:
        print("\nüëã Arr√™t utilisateur")
    except Exception as e:
        print(f"‚ùå Erreur critique: {e}")

if __name__ == "__main__":
    main() 